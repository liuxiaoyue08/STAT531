---
title: "STATS 531 Midterm Project: Association between pm2.5 and temperature difference"
author: "Xiaoyue Liu    UMID:28589009    xiaoyliu@umich.edu"
date: "March 6, 2016"
output: html_document
---

\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\var{\mathrm{Var}}
\newcommand\cov{\mathrm{Cov}}
\newcommand\loglik{\ell}
\newcommand\R{\mathbb{R}}
\newcommand\data[1]{#1^*}
\newcommand\params{\, ; \,}
\newcommand\transpose{\scriptsize{T}}
\newcommand\eqspace{\quad\quad\quad}
\newcommand\lik{\mathscr{L}}
\newcommand\loglik{\ell}
\newcommand\profileloglik[1]{\ell^\mathrm{profile}_#1}
\newcommand\ar{\phi}
\newcommand\ma{\psi}


**<big>1.My experience of air quality and temperature difference</big>**

* In the past 7 years, I was living in Beijing. There has been terrible air pollution since I went to university there. It was most serious from 2010 to 2012. We had to wear facial masks and reduce outdoor activities. Even with those protection, it is still a threat to health.Beijing began monitoring PM 2.5 levels in 2013 amid rising public concerns over pollution in the city.

* From my own experience, those days with a high pm2.5 seemed to have a smaller temperature difference within the day. In summer it brought stuffiness and in winter there seemed less wind.

* May there be relationship between pm2.5 and the temperature difference?
We can analyse using time series methods.


```{r}
x <- read.table(file="https://ms.mcmaster.ca/~bolker/measdata/nycmeas.dat",header=FALSE)
colnames(x)=c('Time', 'Cases')
head(x)
```

**<big>2.Brief summary of the data</big>**

* First let's have a look at the time series of the infectious disease case reports number. 
The black line shows how the case reports number from 1928 to 1968, and the blue line represents the mean for this time series.

```{r echo = F}
plot(Cases~Time,data=x,type="l")
lines(x = x$Time, y = rep(mean(x$Case, na.rm = T), length(x$Time)), col = "blue", lwd = 1.5)
```



**<big>3.Fitting an ARMA model</big>**

* Let's start by fitting a stationary ARMA$(p,q)$ model under the null hypothesis that there is no trend. This hypothesis, which asserts that nothing has substantially changed in this system over the last 150 years, is not entirely unreasonable from looking at the data.


* We seek to fit a stationary Gaussian ARMA(p,q) model with parameter vector $\theta=(\ar_{1:p},\ma_{1:q},\mu,\sigma^2)$ given by
$$ \ar(B)(X_n-\mu) = \ma(B) \epsilon_n,$$
where 
$$\begin{eqnarray}
\mu &=& \E[X_n]
\\
\ar(x)&=&1-\ar_1 x-\dots -\ar_px^p,
\\ 
\ma(x)&=&1+\ma_1 x+\dots +\ma_qx^q, 
\\
\epsilon_n&\sim&\mathrm{ iid }\, N[0,\sigma^2].
\end{eqnarray}$$


* We need to decide where to start in terms of values of $p$ and $q$. Viewed as a way to select a model with reasonable predictive skill from a range of possibilities. 
* Akaike's information criterion **AIC** is given by $$ AIC = -2 \times \loglik(\data{\theta}) + 2D$$ 
* AIC is often useful. Let's tabulate some AIC values for a range of different choices of $p$ and $q$.

```{r, message = FALSE, warning = FALSE, echo = FALSE}
aic_table <- function(data,P,Q){
  table <- matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
       table[p+1,q+1] <- arima(data,order=c(p,0,q))$aic
    }
  }
  dimnames(table) <- list(paste("<b> AR",0:P, "</b>", sep=""),paste("MA",0:Q,sep=""))
  table
}
low_aic_table <- aic_table(x$Low,4,5)
require(knitr)
kable(low_aic_table,digits=2)
```

* We are invited to select the model with the lowest AIC score. Let's see which is the smallest AIC in the table:

```{r, echo = FALSE}
round(min(low_aic_table),2)
```

* From the table we can find that the smallest AIC corresponds to the ARMA(0,0) model, which responds the white noise model:  $$X_n-\mu = \epsilon_n$$ (Here we suppose $\epsilon_n$ 
is the Gaussian white noise where $\epsilon_{1:N} \sim \mbox{IID } N[0,\sigma^2]$)
* However, it is still unclear whether white noise is the true model as AIC is just a way to select a reasonable model.

**<big>4.Compare data with fitted model</big>**

* First we can compare the data with normal distribution to test if a Gaussian white noise model fits. To achieve this goal we can use the hisogram.
```{r, echo = FALSE}
hist(x1$Low, probability = T, main = "Histogram of Low Temperature in January", ylim = c(0,0.06), xlab = "Low Temperature")
```

* The distribution of low temperature in normal looks similar to normal. To see further in detail, we can simulate a normal distribution with the same mean and variance of the data.

```{r, echo=FALSE}
temp_x <- seq(min(x1$Low),max(x1$Low), length = 115)
y <- dnorm(temp_x, mean = mean(x1$Low), sd = sd(x1$Low))
hist(x1$Low, probability = T, main = "Histogram of Low Temperature", ylim = c(0,0.06), xlab = "Low Temperature")
lines(temp_x, y, col = "blue", lwd = 2)
lines(density(x1$Low), col = "black", lwd = 2)
```

* Here the black line and the histogram represent the true distribution of the low temperature data. The blue line is a simulated normal distribution with the same mean and variance of the data.

* We can see that the two distribution are similar in pattern, which means the white noise model is a good choice for this data set.

* We can also compare the autocovariance of the low temperature time series and the white noise model.

* The properties of white noise are:
$$\begin{eqnarray}
\E[\epsilon_n]&=& 0 \\
\cov(\epsilon_m,\epsilon_n) &=& \left\{\begin{array}{ll}
  \sigma^2, & \mbox{if $m=n$} \\
   0, & \mbox{if $m\neq n$} \end{array}\right. ,
\end{eqnarray}$$

* The Autocovraiance for white noise model is: 
$$ \begin{eqnarray} \gamma_{h} = \left\{ \begin{array}{ll} \sigma^2 & \textrm{if $h=0$}\\ 0 & \textrm{if $h\neq0$}\end{array} \right. \end{eqnarray} $$

* Let's compute the acf and variance of the low temperature
```{r}
acf(x1$Low, type = "covariance", plot = F)
var(x1$Low)
```

* We can see that the acf of the low temperature is 53.367, which is close to the variance of the low temperature, which is 53.83923. It means that the white noise model fits the low temperature data really well. 

* Here is the acf of the data from the time series:
```{r}
summary(acf(x1$Low)$acf)
```

* Then we simulate a time series with 115 time points for the ARMA(0,0) model (the white noise model) and get its acf:
```{r}
set.seed(123456789)
ARMA_0_0 <- arima.sim(list(c(0,0,0)),115)
summary(acf(ARMA_0_0)$acf)
```

* We can see that the two acf plots are similar. Again it shows the white noise model does fit the data quite well.


**<big>5.Summary</big>**

* We find ARMA(0,0)(Gaussian white noise) model fits the Ann Arbor January Low temperature time series. We compute the autocovariance of the data and compare the data with a normal distribution. All of these results suggest that ARMA(0,0)(Gaussian white noise) model is a good fit for the data.

* As white noise model fits the data well, it means that there's no pattern, just random variation in the low temperature in January for Ann Arbor, showing that we can not predict next year's low temperature based on data from previous years. Maybe one year is too long for predicting the temperature. Last year(2014) it was quite cold in January and there was lot of snow, however this year it is too warm for Michigan winter, resulting in little snow and difficulty for snow resorts to make snow.


